{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67c20c9",
   "metadata": {},
   "source": [
    "## Autograd\n",
    " \n",
    "Autograd helps in performing automatic differentiation when training a model and using a optimization algorithm like GD.\n",
    "To enable automatic differentiation we have to define the tensor with this attribute that is requires_grad = True when defining the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae0ef5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0226a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(3.0, requires_grad= True) # leaf tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7683631",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3eca313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "426a842b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfa31b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward()   #backward() computes gradients using backpropagation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "408e97be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad   # and .grad stores the gradient of the output with respect to a leaf tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "586db203",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example Number 2 \n",
    "\n",
    "x = torch.randn((4,4), requires_grad = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "312c313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x+ 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14780742",
   "metadata": {},
   "outputs": [],
   "source": [
    "z =( y**2 /3)\n",
    "# z =( y**2 /3).mean() this  will give you a scalar output and  is most used un training neural networks as the grad\n",
    "# ient values are small and easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))\n",
    "# z.backward() used this when z is scalar ie herer using the mean method\n",
    " # when the output is not a scalar, we need to provide the gradient argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77aaa050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.1285, 21.1137, 21.0638, 20.4828],\n",
       "        [21.0922, 22.0769, 21.3843, 20.4889],\n",
       "        [21.5277, 20.7600, 21.1099, 21.3451],\n",
       "        [20.8075, 21.0698, 20.7387, 22.5073]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c715171",
   "metadata": {},
   "source": [
    "### Clearing Gradients \n",
    "This is a important concept that help us to clear the gradients and make them zero \n",
    "\n",
    "why do we do this ?\n",
    "\n",
    "We do this to becoz the gradient each and everytime we run a forward pass the gradients gets stored and when again the forward pass is done the gradient will add up which will affect the output of the model \n",
    "\n",
    "eg:- follow this \n",
    "1. forward pass  the grad after we calculate the backward would be 4 for exmaple\n",
    "2. when we again run the forward and the backward pass the gradient would be 8 \n",
    "\n",
    "why is that ?\n",
    "the resultant gradient after the backward pass which was 4 will add up to the gradient which was calculated before would add up to result 4 \n",
    "\n",
    "this would result in large gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid this we can do the following\n",
    "\n",
    "x1 = torch.tensor(5.0, requires_grad=True)\n",
    "y1 = x1**3\n",
    "\n",
    "# this abpove will line of code is forward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.backward()  # this is the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ac982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad # to see the change in the grad value before the we make it zero you have to run the above two code cells \n",
    "# again and then run this again to see the accumulation effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95ba2acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad.zero_() # this will reset the gradients to zero hence solving the accumulation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aef240",
   "metadata": {},
   "source": [
    "### Removing the gradient tracking \n",
    "\n",
    "This is done in a scenario when the we no longer the need the gradient ie when the training the model the has completed and now we want to make prediction or evaluate the model.\n",
    "\n",
    "we can use the following functios to achieve this task \n",
    "\n",
    "1. require_grad_(False)\n",
    "2. detach()\n",
    "3. with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17fb97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.tensor(5.0, requires_grad=True)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cad783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.requires_grad_(False)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd2ca256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8073, -0.3294, -0.4043, -1.2758],\n",
       "        [-0.3617,  1.1154,  0.0764, -1.2666],\n",
       "        [ 0.2915, -0.8600, -0.3351,  0.0176],\n",
       "        [-0.7887, -0.3953, -0.8919,  1.7610]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2 \n",
    "\n",
    "x2 = x.detach()\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f46ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
